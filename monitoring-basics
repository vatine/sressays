Monitoring is easy. Good monitoring is surprisingly hard. Most people
looking at service and system monitoring go "ah, it is a thing,
therefore I shall monitor it and set alerts based on it". This is, I
have found, not really the best way to go about things.

Monitor the things you actually care about. And set alerts based on
this monitoring. If you run a "public-facing web service" things you
care about are probably things like "4xx responses", "2xx responses",
"5xx responses", "incoming request rate" and "response latency".

Many people think they care about CPU usage, system load, disk
fullness, but they don't, really. They care about the end-users'
perception of responsiveness and up-ness. But things like system load
and how full is the disk are easy to measure, so those get measured
and all sorts of interesting theories how these respond to the things
you actually care about start happening.

Now, I am not saying you should not measure those other things. You
probably should, but only because having timelines for these may make
it easier to quickly troubleshoot once one of the things you actually
care about go outside the limit where you think it is fine.

Then, once you're monitoring the things you actually care about, you should carefully consider when those values are "OK" and "bad". What's a suitable 99th percentile latency for "serve the home page"? What's an acceptable rate of "5xx status per incoming request" (it's probably higher than 0, and probably vastly lower than 1, maybe 1 in every 1000?). KTune your alerting to this.

If you're monitoring things like disk and what have you, you could
then set oethr thresholds up, for doing things like "file a ticket to
expand disk, once tehres only 7 days left until the disk is full" and
maybe "fire off an automatic clean-up job, before filing the ticket,
then re-assess the extrapolation of disk filling up".